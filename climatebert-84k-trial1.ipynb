{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14615856,"sourceType":"datasetVersion","datasetId":9335957}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:26:44.128145Z","iopub.execute_input":"2026-01-25T17:26:44.128345Z","iopub.status.idle":"2026-01-25T17:26:45.305050Z","shell.execute_reply.started":"2026-01-25T17:26:44.128324Z","shell.execute_reply":"2026-01-25T17:26:45.304210Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/climate-dataset-84k/export.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Cell 1: Dependencies","metadata":{}},{"cell_type":"code","source":"!pip -q install -U transformers datasets accelerate scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:26:49.594865Z","iopub.execute_input":"2026-01-25T17:26:49.595591Z","iopub.status.idle":"2026-01-25T17:27:10.226851Z","shell.execute_reply.started":"2026-01-25T17:26:49.595551Z","shell.execute_reply":"2026-01-25T17:27:10.226011Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install iterative-stratification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:27:46.027284Z","iopub.execute_input":"2026-01-25T17:27:46.028085Z","iopub.status.idle":"2026-01-25T17:27:49.363447Z","shell.execute_reply.started":"2026-01-25T17:27:46.028048Z","shell.execute_reply":"2026-01-25T17:27:49.362474Z"}},"outputs":[{"name":"stdout","text":"Collecting iterative-stratification\n  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (2.0.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (1.8.0)\nRequirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\nDownloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.9\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Cell 2: Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch import nn\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    DataCollatorWithPadding,\n    set_seed\n)\n\nSEED = 42\nset_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:28:54.959977Z","iopub.execute_input":"2026-01-25T17:28:54.960840Z","iopub.status.idle":"2026-01-25T17:29:24.071787Z","shell.execute_reply.started":"2026-01-25T17:28:54.960802Z","shell.execute_reply":"2026-01-25T17:29:24.070953Z"}},"outputs":[{"name":"stderr","text":"2026-01-25 17:29:08.818648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769362149.023735      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769362149.090346      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769362149.600184      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769362149.600229      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769362149.600232      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769362149.600234      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Cell 3: Load dataset","metadata":{}},{"cell_type":"code","source":"CSV_PATH = \"/kaggle/input/climate-dataset-84k/export.csv\"  # <-- change this\n\ndf = pd.read_csv(CSV_PATH)\nprint(df.shape)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:48:44.100835Z","iopub.execute_input":"2026-01-25T17:48:44.101281Z","iopub.status.idle":"2026-01-25T17:48:46.263631Z","shell.execute_reply.started":"2026-01-25T17:48:44.101250Z","shell.execute_reply":"2026-01-25T17:48:46.263046Z"}},"outputs":[{"name":"stdout","text":"(78401, 38)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                   title  publication_year  \\\n0      Modelling Stand Dynamics after Selective Loggi...            2012.0   \n1      Mathematics, Politics, and Greenhouse Gas Inte...            2003.0   \n2      The Contribution of Rural Development Programm...            2012.0   \n3      Negotiating greenhouse abatement and the theor...            1999.0   \n4      Is the introduction of a carbon tax a ‘teachab...            2012.0   \n...                                                  ...               ...   \n78396  Carbon Neutrality as Leverage in Transitioning...            2006.0   \n78397  Fiscal mechanisms to promote CO2 for enhanced ...            2005.0   \n78398  Experimental comparison between R152a and R134...            2015.0   \n78399  BRIEF COMMUNICATION: Genetic control of the ru...            2015.0   \n78400  Cost Effective and Environmentally Safe Emissi...            2010.0   \n\n                                    doi  \\\n0                                   NaN   \n1                  10.5951/mt.96.9.0646   \n2          10.1007/978-3-642-22266-5_23   \n3           10.1007/978-94-015-9169-0_3   \n4                                   NaN   \n...                                 ...   \n78396                               NaN   \n78397  10.1016/b978-008044704-9/50188-9   \n78398    10.1016/j.ijrefrig.2015.06.021   \n78399                               NaN   \n78400       10.1007/978-3-642-03735-1_5   \n\n                                            institutions    idx  \\\n0                                                    NaN      0   \n1           California State University, Dominguez Hills      1   \n2      Istituto Superiore per la Protezione e la Rice...      2   \n3      Australian Bureau of Agricultural and Resource...      3   \n4                                                    NaN      4   \n...                                                  ...    ...   \n78396                                                NaN  78396   \n78397                                CO2-Norway (Norway)  78397   \n78398  Universitat Politècnica de València; Jaume I U...  78398   \n78399                                         AgResearch  78399   \n78400  International Institute for Applied Systems An...  78400   \n\n                                                 authors  openalex_id  \\\n0                                                    NaN   W999667810   \n1                                         Stan Yoshinobu   W999644455   \n2      Rocío Dánica Cóndor; Marina Vitullo; Domenico ...   W999485533   \n3                           Mike Hinchy; Brian S. Fisher   W998914803   \n4                 Liam Smith; Bas Verplanken; Jim Curtis   W998247206   \n...                                                  ...          ...   \n78396    Melanie Dubin; Magdalena Szpala; Tamara Connell  W1001986490   \n78397  David L. Coleman; Carl-W. Hustad; J. Michael A...   W100136754   \n78398  Rodrigo Llopis; R. Cabello; E. Torrella; Danie...  W1001294588   \n78399  Pinares-Patino Cs; Wood Gr; Hickey Sm; Kirk Mr...  W1000733675   \n78400  G. Fischer; Y. Ermoliev; T. Ermolieva; M. Jona...   W100066195   \n\n                                                abstract    meth|0     reg|0  \\\n0      Abstract: Forest degradation and biomass damag...  0.990234  0.020004   \n1      While reading the newspaper one morning in spr...  0.640137  0.010002   \n2      The Health Check reform, reinforcing the Commo...  0.620117  0.119995   \n3      The initial qualified commitment of Annex 1 co...  0.529785  0.020004   \n4      With the arrival of the carbon tax earlier thi...  0.439941  0.010002   \n...                                                  ...       ...       ...   \n78396  Climate change is one of the most pressing env...  0.040009  0.020004   \n78397  The use of carbon dioxide (CO2) for enhanced o...  0.020004  0.080017   \n78398  Abstract The EU Regulation 517/2014 has recent...  0.939941  0.029999   \n78399  Introduction Methane is a waste product genera...  0.750000  0.020004   \n78400  The aim of this paper is to analyze robust cos...  0.970215  0.010002   \n\n       ...     gov|0     ins|3    meth|1    econ|2     lvl|0     sec|5  \\\n0      ...  0.010002  0.020004  0.000000  0.010002  1.000000  0.010002   \n1      ...  0.000000  0.919922  0.239990  0.010002  0.010002  0.010002   \n2      ...  0.049988  0.959961  0.239990  0.130005  0.010002  0.020004   \n3      ...  0.020004  0.310059  0.320068  0.010002  1.000000  0.010002   \n4      ...  0.000000  0.010002  0.439941  0.020004  0.010002  0.010002   \n...    ...       ...       ...       ...       ...       ...       ...   \n78396  ...  0.010002  0.010002  0.930176  0.000000  1.000000  0.010002   \n78397  ...  0.020004  0.130005  0.970215  0.020004  0.959961  0.010002   \n78398  ...  0.020004  0.090027  0.020004  0.029999  1.000000  0.010002   \n78399  ...  0.020004  0.059998  0.140015  0.020004  0.250000  0.020004   \n78400  ...  0.010002  0.580078  0.010002  0.010002  0.189941  0.010002   \n\n          sec|1     sec|2     ins|4     sec|3  \n0      0.020004  0.010002  0.000000  0.029999  \n1      0.010002  0.010002  0.010002  0.020004  \n2      0.020004  0.010002  0.000000  0.059998  \n3      0.010002  0.010002  0.990234  0.020004  \n4      0.020004  0.010002  0.010002  0.109985  \n...         ...       ...       ...       ...  \n78396  0.000000  0.040009  0.010002  0.059998  \n78397  0.010002  0.229980  0.180054  0.970215  \n78398  0.229980  0.020004  0.049988  0.150024  \n78399  0.020004  0.010002  0.010002  0.040009  \n78400  0.010002  0.010002  0.540039  0.020004  \n\n[78401 rows x 38 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>publication_year</th>\n      <th>doi</th>\n      <th>institutions</th>\n      <th>idx</th>\n      <th>authors</th>\n      <th>openalex_id</th>\n      <th>abstract</th>\n      <th>meth|0</th>\n      <th>reg|0</th>\n      <th>...</th>\n      <th>gov|0</th>\n      <th>ins|3</th>\n      <th>meth|1</th>\n      <th>econ|2</th>\n      <th>lvl|0</th>\n      <th>sec|5</th>\n      <th>sec|1</th>\n      <th>sec|2</th>\n      <th>ins|4</th>\n      <th>sec|3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Modelling Stand Dynamics after Selective Loggi...</td>\n      <td>2012.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>W999667810</td>\n      <td>Abstract: Forest degradation and biomass damag...</td>\n      <td>0.990234</td>\n      <td>0.020004</td>\n      <td>...</td>\n      <td>0.010002</td>\n      <td>0.020004</td>\n      <td>0.000000</td>\n      <td>0.010002</td>\n      <td>1.000000</td>\n      <td>0.010002</td>\n      <td>0.020004</td>\n      <td>0.010002</td>\n      <td>0.000000</td>\n      <td>0.029999</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mathematics, Politics, and Greenhouse Gas Inte...</td>\n      <td>2003.0</td>\n      <td>10.5951/mt.96.9.0646</td>\n      <td>California State University, Dominguez Hills</td>\n      <td>1</td>\n      <td>Stan Yoshinobu</td>\n      <td>W999644455</td>\n      <td>While reading the newspaper one morning in spr...</td>\n      <td>0.640137</td>\n      <td>0.010002</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.919922</td>\n      <td>0.239990</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.020004</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Contribution of Rural Development Programm...</td>\n      <td>2012.0</td>\n      <td>10.1007/978-3-642-22266-5_23</td>\n      <td>Istituto Superiore per la Protezione e la Rice...</td>\n      <td>2</td>\n      <td>Rocío Dánica Cóndor; Marina Vitullo; Domenico ...</td>\n      <td>W999485533</td>\n      <td>The Health Check reform, reinforcing the Commo...</td>\n      <td>0.620117</td>\n      <td>0.119995</td>\n      <td>...</td>\n      <td>0.049988</td>\n      <td>0.959961</td>\n      <td>0.239990</td>\n      <td>0.130005</td>\n      <td>0.010002</td>\n      <td>0.020004</td>\n      <td>0.020004</td>\n      <td>0.010002</td>\n      <td>0.000000</td>\n      <td>0.059998</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Negotiating greenhouse abatement and the theor...</td>\n      <td>1999.0</td>\n      <td>10.1007/978-94-015-9169-0_3</td>\n      <td>Australian Bureau of Agricultural and Resource...</td>\n      <td>3</td>\n      <td>Mike Hinchy; Brian S. Fisher</td>\n      <td>W998914803</td>\n      <td>The initial qualified commitment of Annex 1 co...</td>\n      <td>0.529785</td>\n      <td>0.020004</td>\n      <td>...</td>\n      <td>0.020004</td>\n      <td>0.310059</td>\n      <td>0.320068</td>\n      <td>0.010002</td>\n      <td>1.000000</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.990234</td>\n      <td>0.020004</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Is the introduction of a carbon tax a ‘teachab...</td>\n      <td>2012.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>Liam Smith; Bas Verplanken; Jim Curtis</td>\n      <td>W998247206</td>\n      <td>With the arrival of the carbon tax earlier thi...</td>\n      <td>0.439941</td>\n      <td>0.010002</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.010002</td>\n      <td>0.439941</td>\n      <td>0.020004</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.020004</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.109985</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>78396</th>\n      <td>Carbon Neutrality as Leverage in Transitioning...</td>\n      <td>2006.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78396</td>\n      <td>Melanie Dubin; Magdalena Szpala; Tamara Connell</td>\n      <td>W1001986490</td>\n      <td>Climate change is one of the most pressing env...</td>\n      <td>0.040009</td>\n      <td>0.020004</td>\n      <td>...</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.930176</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.010002</td>\n      <td>0.000000</td>\n      <td>0.040009</td>\n      <td>0.010002</td>\n      <td>0.059998</td>\n    </tr>\n    <tr>\n      <th>78397</th>\n      <td>Fiscal mechanisms to promote CO2 for enhanced ...</td>\n      <td>2005.0</td>\n      <td>10.1016/b978-008044704-9/50188-9</td>\n      <td>CO2-Norway (Norway)</td>\n      <td>78397</td>\n      <td>David L. Coleman; Carl-W. Hustad; J. Michael A...</td>\n      <td>W100136754</td>\n      <td>The use of carbon dioxide (CO2) for enhanced o...</td>\n      <td>0.020004</td>\n      <td>0.080017</td>\n      <td>...</td>\n      <td>0.020004</td>\n      <td>0.130005</td>\n      <td>0.970215</td>\n      <td>0.020004</td>\n      <td>0.959961</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.229980</td>\n      <td>0.180054</td>\n      <td>0.970215</td>\n    </tr>\n    <tr>\n      <th>78398</th>\n      <td>Experimental comparison between R152a and R134...</td>\n      <td>2015.0</td>\n      <td>10.1016/j.ijrefrig.2015.06.021</td>\n      <td>Universitat Politècnica de València; Jaume I U...</td>\n      <td>78398</td>\n      <td>Rodrigo Llopis; R. Cabello; E. Torrella; Danie...</td>\n      <td>W1001294588</td>\n      <td>Abstract The EU Regulation 517/2014 has recent...</td>\n      <td>0.939941</td>\n      <td>0.029999</td>\n      <td>...</td>\n      <td>0.020004</td>\n      <td>0.090027</td>\n      <td>0.020004</td>\n      <td>0.029999</td>\n      <td>1.000000</td>\n      <td>0.010002</td>\n      <td>0.229980</td>\n      <td>0.020004</td>\n      <td>0.049988</td>\n      <td>0.150024</td>\n    </tr>\n    <tr>\n      <th>78399</th>\n      <td>BRIEF COMMUNICATION: Genetic control of the ru...</td>\n      <td>2015.0</td>\n      <td>NaN</td>\n      <td>AgResearch</td>\n      <td>78399</td>\n      <td>Pinares-Patino Cs; Wood Gr; Hickey Sm; Kirk Mr...</td>\n      <td>W1000733675</td>\n      <td>Introduction Methane is a waste product genera...</td>\n      <td>0.750000</td>\n      <td>0.020004</td>\n      <td>...</td>\n      <td>0.020004</td>\n      <td>0.059998</td>\n      <td>0.140015</td>\n      <td>0.020004</td>\n      <td>0.250000</td>\n      <td>0.020004</td>\n      <td>0.020004</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.040009</td>\n    </tr>\n    <tr>\n      <th>78400</th>\n      <td>Cost Effective and Environmentally Safe Emissi...</td>\n      <td>2010.0</td>\n      <td>10.1007/978-3-642-03735-1_5</td>\n      <td>International Institute for Applied Systems An...</td>\n      <td>78400</td>\n      <td>G. Fischer; Y. Ermoliev; T. Ermolieva; M. Jona...</td>\n      <td>W100066195</td>\n      <td>The aim of this paper is to analyze robust cos...</td>\n      <td>0.970215</td>\n      <td>0.010002</td>\n      <td>...</td>\n      <td>0.010002</td>\n      <td>0.580078</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.189941</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.010002</td>\n      <td>0.540039</td>\n      <td>0.020004</td>\n    </tr>\n  </tbody>\n</table>\n<p>78401 rows × 38 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Cell 4 — Identify label columns automatically\n\nThis detects all columns like econ|0, reg|2, sec|6, etc.","metadata":{}},{"cell_type":"code","source":"meta_cols = {\"title\", \"publication_year\", \"doi\", \"institutions\", \"idx\", \"authors\", \"openalex_id\", \"abstract\"}\nlabel_cols = [c for c in df.columns if c not in meta_cols and \"|\" in c]\n\n# keep only columns that look like category|digit\nlabel_cols = [c for c in label_cols if re.search(r\"\\|\\d+$\", c)]\nlabel_cols = sorted(label_cols)\n\nprint(\"Num label columns:\", len(label_cols))\nprint(label_cols[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:49:32.519914Z","iopub.execute_input":"2026-01-25T17:49:32.520668Z","iopub.status.idle":"2026-01-25T17:49:32.525970Z","shell.execute_reply.started":"2026-01-25T17:49:32.520637Z","shell.execute_reply":"2026-01-25T17:49:32.525161Z"}},"outputs":[{"name":"stdout","text":"Num label columns: 30\n['econ|0', 'econ|1', 'econ|2', 'econ|3', 'edu|0', 'ev|0', 'ev|1', 'gov|0', 'gov|1', 'gov|2', 'ins|0', 'ins|1', 'ins|2', 'ins|3', 'ins|4', 'lvl|0', 'lvl|1', 'lvl|2', 'meth|0', 'meth|1']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Cell 5 — Subsample 20,000 rows + make text field","metadata":{}},{"cell_type":"code","source":"# Keep only rows that have some text\ndf[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\ndf[\"abstract\"] = df[\"abstract\"].fillna(\"\").astype(str)\n\n# Simple abstract cleaning (remove extra whitespace)\ndef clean_text(s):\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndf[\"title\"] = df[\"title\"].apply(clean_text)\ndf[\"abstract\"] = df[\"abstract\"].apply(clean_text)\n\n# Combine title + abstract\ndf[\"text\"] = (df[\"title\"] + \" [SEP] \" + df[\"abstract\"]).str.strip()\n\n# Ensure labels are numeric 0/1\nfor c in label_cols:\n    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int).clip(0, 1)\n\n# Remove empty text rows\ndf = df[df[\"text\"].str.len() > 5].reset_index(drop=True)\n\n# Subsample\nN = 20000\nif len(df) > N:\n    df = df.sample(n=N, random_state=SEED).reset_index(drop=True)\n\nprint(\"After subsample:\", df.shape)\ndf[[\"text\"] + label_cols[:5]].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:50:20.078358Z","iopub.execute_input":"2026-01-25T17:50:20.079092Z","iopub.status.idle":"2026-01-25T17:50:27.107086Z","shell.execute_reply.started":"2026-01-25T17:50:20.079059Z","shell.execute_reply":"2026-01-25T17:50:27.106350Z"}},"outputs":[{"name":"stdout","text":"After subsample: (20000, 39)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                text  econ|0  econ|1  econ|2  \\\n0  Does urban vegetation enhance carbon sequestra...       0       0       0   \n1  Exploring the driving forces of distributed en...       0       0       0   \n2  Energy consumption analysis for CO2 separation...       0       0       0   \n3  Economic and employment effects of China's pow...       0       0       0   \n4  Leveraging Carbon Markets for Green Buildings:...       0       0       0   \n\n   econ|3  edu|0  \n0       0      0  \n1       0      0  \n2       0      0  \n3       0      0  \n4       0      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>econ|0</th>\n      <th>econ|1</th>\n      <th>econ|2</th>\n      <th>econ|3</th>\n      <th>edu|0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Does urban vegetation enhance carbon sequestra...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Exploring the driving forces of distributed en...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Energy consumption analysis for CO2 separation...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Economic and employment effects of China's pow...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Leveraging Carbon Markets for Green Buildings:...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Cell 6 — Multi-label stratified split (train/val/test)","metadata":{}},{"cell_type":"code","source":"Y = df[label_cols].values\n\nmsss1 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\ntrain_idx, temp_idx = next(msss1.split(df, Y))\n\ndf_train = df.iloc[train_idx].reset_index(drop=True)\ndf_temp  = df.iloc[temp_idx].reset_index(drop=True)\n\nY_temp = df_temp[label_cols].values\nmsss2 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=SEED)\nval_idx, test_idx = next(msss2.split(df_temp, Y_temp))\n\ndf_val  = df_temp.iloc[val_idx].reset_index(drop=True)\ndf_test = df_temp.iloc[test_idx].reset_index(drop=True)\n\nprint(\"Train:\", df_train.shape, \"Val:\", df_val.shape, \"Test:\", df_test.shape)\n\n# sanity: label prevalence\nprint(\"Avg labels per sample (train):\", df_train[label_cols].sum(axis=1).mean())\nprint(\"Avg labels per sample (val):\", df_val[label_cols].sum(axis=1).mean())\nprint(\"Avg labels per sample (test):\", df_test[label_cols].sum(axis=1).mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:50:56.705862Z","iopub.execute_input":"2026-01-25T17:50:56.706531Z","iopub.status.idle":"2026-01-25T17:50:57.072089Z","shell.execute_reply.started":"2026-01-25T17:50:56.706500Z","shell.execute_reply":"2026-01-25T17:50:57.071421Z"}},"outputs":[{"name":"stdout","text":"Train: (16000, 39) Val: (2000, 39) Test: (2000, 39)\nAvg labels per sample (train): 0.8450625\nAvg labels per sample (val): 0.843\nAvg labels per sample (test): 0.8465\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Cell 7 — Baseline model (TF-IDF + One-vs-Rest Logistic Regression)","metadata":{}},{"cell_type":"code","source":"X_train = df_train[\"text\"].tolist()\nX_val   = df_val[\"text\"].tolist()\nX_test  = df_test[\"text\"].tolist()\n\ny_train = df_train[label_cols].values\ny_val   = df_val[label_cols].values\ny_test  = df_test[label_cols].values\n\ntfidf = TfidfVectorizer(max_features=100000, ngram_range=(1,2), min_df=2)\nXtr = tfidf.fit_transform(X_train)\nXva = tfidf.transform(X_val)\nXte = tfidf.transform(X_test)\n\nclf = OneVsRestClassifier(LogisticRegression(max_iter=200, n_jobs=-1))\nclf.fit(Xtr, y_train)\n\nval_pred = (clf.predict_proba(Xva) >= 0.5).astype(int)\nprint(\"Baseline TF-IDF OVR Logistic\")\nprint(\"Macro-F1:\", f1_score(y_val, val_pred, average=\"macro\", zero_division=0))\nprint(\"Micro-F1:\", f1_score(y_val, val_pred, average=\"micro\", zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:51:32.789977Z","iopub.execute_input":"2026-01-25T17:51:32.790614Z","iopub.status.idle":"2026-01-25T17:52:14.403517Z","shell.execute_reply.started":"2026-01-25T17:51:32.790584Z","shell.execute_reply":"2026-01-25T17:52:14.401716Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 2 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 3 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 4 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 5 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 7 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 9 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 18 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 19 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 20 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 21 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 22 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 23 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 24 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 25 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n  warnings.warn(msg, category=FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"Baseline TF-IDF OVR Logistic\nMacro-F1: 0.062358516057566295\nMicro-F1: 0.5128205128205128\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 27 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 28 is present in all training examples.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py:90: UserWarning: Label not 29 is present in all training examples.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Cell 8 — Tokenizer & Model","metadata":{}},{"cell_type":"code","source":"MODEL_CANDIDATES = [\n    \"climatebert/distilroberta-base-climate-f\",\n    \"climatebert/distilroberta-base-climate-detection\",\n    \"distilroberta-base\",  # fallback if ClimateBERT name differs on Kaggle\n]\n\nMODEL_NAME = None\ntokenizer = None\nmodel = None\n\nfor cand in MODEL_CANDIDATES:\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(cand, use_fast=True)\n        model = AutoModelForSequenceClassification.from_pretrained(\n            cand,\n            num_labels=len(label_cols),\n            problem_type=\"multi_label_classification\"\n        )\n        MODEL_NAME = cand\n        break\n    except Exception as e:\n        print(\"Failed:\", cand, \"|\", str(e)[:120])\n\nprint(\"Using model:\", MODEL_NAME)\nmodel.to(device);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:53:03.553219Z","iopub.execute_input":"2026-01-25T17:53:03.553556Z","iopub.status.idle":"2026-01-25T17:53:09.408216Z","shell.execute_reply.started":"2026-01-25T17:53:03.553530Z","shell.execute_reply":"2026-01-25T17:53:09.407634Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ac968accf34f89bf88904e147b4912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6534b230b1224034ba0ee6406c4f7065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af8cd46587df440596dc80a56cc65c58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0905b2b44db9477b98e584817d4db79b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"223e15fcfda24655a1645df0b7b65c02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3386d3581224a7a98b51fb23d3c0508"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc42e75fb4f42319360eec302a10686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d2df2c0404437a889c570705830912"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using model: climatebert/distilroberta-base-climate-f\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Cell 9 — Build HuggingFace Datasets","metadata":{}},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(df_train[[\"text\"] + label_cols])\nval_ds   = Dataset.from_pandas(df_val[[\"text\"] + label_cols])\ntest_ds  = Dataset.from_pandas(df_test[[\"text\"] + label_cols])\n\ndef tokenize_batch(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n\ntrain_ds = train_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\nval_ds   = val_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\ntest_ds  = test_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n\n# Transformers expects labels in a \"labels\" field\ndef to_labels(batch):\n    batch[\"labels\"] = [ [batch[c][i] for c in label_cols] for i in range(len(batch[label_cols[0]])) ]\n    for c in label_cols:\n        del batch[c]\n    return batch\n\ntrain_ds = train_ds.map(to_labels, batched=True)\nval_ds   = val_ds.map(to_labels, batched=True)\ntest_ds  = test_ds.map(to_labels, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:54:12.699799Z","iopub.execute_input":"2026-01-25T17:54:12.700449Z","iopub.status.idle":"2026-01-25T17:54:23.215450Z","shell.execute_reply.started":"2026-01-25T17:54:12.700412Z","shell.execute_reply":"2026-01-25T17:54:23.214633Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75716b3854c442ea02d83306a822480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3353a10ecb8340dabebf5aad14acf641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc6c3388ef44781b89f9b80e419df9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d0a7ab7df2b412ca9673c4de44ddb36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"372512ab499c4241978abaedcbc07c9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c669f520e844a01b06fa7916aa18ebc"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# Cell 10 — Class imbalance handling (pos_weight)","metadata":{}},{"cell_type":"code","source":"y_train = df_train[label_cols].values\npos = y_train.sum(axis=0)\nneg = y_train.shape[0] - pos\npos_weight = (neg / (pos + 1e-6)).astype(np.float32)\npos_weight = np.clip(pos_weight, 1.0, 50.0)  # cap extreme weights\n\npos_weight_t = torch.tensor(pos_weight).to(device)\nprint(\"pos_weight sample:\", pos_weight[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:55:17.578947Z","iopub.execute_input":"2026-01-25T17:55:17.579724Z","iopub.status.idle":"2026-01-25T17:55:17.587386Z","shell.execute_reply.started":"2026-01-25T17:55:17.579693Z","shell.execute_reply":"2026-01-25T17:55:17.586740Z"}},"outputs":[{"name":"stdout","text":"pos_weight sample: [50.       50.       50.       50.       50.       50.        5.751055\n 50.       50.       50.      ]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Cell 11 — Custom Trainer (BCEWithLogitsLoss + pos_weight)","metadata":{}},{"cell_type":"code","source":"class MultilabelTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        labels = labels.float().to(logits.device)\n        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n        loss = loss_fn(logits, labels)\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:56:14.980006Z","iopub.execute_input":"2026-01-25T17:56:14.980367Z","iopub.status.idle":"2026-01-25T17:56:14.985398Z","shell.execute_reply.started":"2026-01-25T17:56:14.980340Z","shell.execute_reply":"2026-01-25T17:56:14.984563Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Cell 12 — Metrics + thresholding","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = sigmoid(logits)\n    preds = (probs >= 0.5).astype(int)\n\n    macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n    micro_f1 = f1_score(labels, preds, average=\"micro\", zero_division=0)\n    macro_p  = precision_score(labels, preds, average=\"macro\", zero_division=0)\n    macro_r  = recall_score(labels, preds, average=\"macro\", zero_division=0)\n\n    return {\n        \"macro_f1\": macro_f1,\n        \"micro_f1\": micro_f1,\n        \"macro_precision\": macro_p,\n        \"macro_recall\": macro_r\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:57:15.226486Z","iopub.execute_input":"2026-01-25T17:57:15.227097Z","iopub.status.idle":"2026-01-25T17:57:15.232270Z","shell.execute_reply.started":"2026-01-25T17:57:15.227067Z","shell.execute_reply":"2026-01-25T17:57:15.231580Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Cell 13 — TrainingArguments (P100-friendly)","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"climatebert_policy_multilabel\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    gradient_accumulation_steps=2,   # effective batch size 32\n    num_train_epochs=3,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    save_strategy=\"steps\",\n    save_steps=200,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"micro_f1\",\n    greater_is_better=True,\n    fp16=torch.cuda.is_available(),\n    logging_steps=50,\n    report_to=\"none\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:58:53.027922Z","iopub.execute_input":"2026-01-25T17:58:53.028608Z","iopub.status.idle":"2026-01-25T17:58:53.058646Z","shell.execute_reply.started":"2026-01-25T17:58:53.028570Z","shell.execute_reply":"2026-01-25T17:58:53.057949Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Cell 14 — Train","metadata":{}},{"cell_type":"code","source":"trainer = MultilabelTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T17:59:46.979697Z","iopub.execute_input":"2026-01-25T17:59:46.980350Z","iopub.status.idle":"2026-01-25T18:10:17.039935Z","shell.execute_reply.started":"2026-01-25T17:59:46.980319Z","shell.execute_reply":"2026-01-25T18:10:17.039023Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/2054891009.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultilabelTrainer.__init__`. Use `processing_class` instead.\n  trainer = MultilabelTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1500/1500 10:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n      <th>Micro F1</th>\n      <th>Macro Precision</th>\n      <th>Macro Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.404300</td>\n      <td>0.384235</td>\n      <td>0.042462</td>\n      <td>0.385000</td>\n      <td>0.040897</td>\n      <td>0.099882</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.291900</td>\n      <td>0.275244</td>\n      <td>0.080850</td>\n      <td>0.423013</td>\n      <td>0.059019</td>\n      <td>0.192722</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.238500</td>\n      <td>0.225246</td>\n      <td>0.111088</td>\n      <td>0.524319</td>\n      <td>0.081243</td>\n      <td>0.235679</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.206800</td>\n      <td>0.200127</td>\n      <td>0.130593</td>\n      <td>0.586885</td>\n      <td>0.096409</td>\n      <td>0.253041</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.191800</td>\n      <td>0.192276</td>\n      <td>0.135900</td>\n      <td>0.614133</td>\n      <td>0.104861</td>\n      <td>0.216638</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.178100</td>\n      <td>0.177177</td>\n      <td>0.134539</td>\n      <td>0.612815</td>\n      <td>0.101764</td>\n      <td>0.254855</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.165800</td>\n      <td>0.176185</td>\n      <td>0.144908</td>\n      <td>0.643201</td>\n      <td>0.108627</td>\n      <td>0.248314</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1500, training_loss=0.2714220059712728, metrics={'train_runtime': 629.5019, 'train_samples_per_second': 76.251, 'train_steps_per_second': 2.383, 'total_flos': 3180805079040000.0, 'train_loss': 0.2714220059712728, 'epoch': 3.0})"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"# Cell 15 — Tune threshold on validation (optional but recommended)","metadata":{}},{"cell_type":"code","source":"val_out = trainer.predict(val_ds)\nval_probs = sigmoid(val_out.predictions)\nval_labels = val_out.label_ids\n\nbest_t, best_micro = 0.5, -1\nfor t in np.linspace(0.1, 0.9, 17):\n    preds = (val_probs >= t).astype(int)\n    micro = f1_score(val_labels, preds, average=\"micro\", zero_division=0)\n    if micro > best_micro:\n        best_micro = micro\n        best_t = float(t)\n\nprint(\"Best threshold:\", best_t, \"Best Val Micro-F1:\", best_micro)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T18:11:01.043583Z","iopub.execute_input":"2026-01-25T18:11:01.043908Z","iopub.status.idle":"2026-01-25T18:11:09.096472Z","shell.execute_reply.started":"2026-01-25T18:11:01.043866Z","shell.execute_reply":"2026-01-25T18:11:09.095919Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Best threshold: 0.65 Best Val Micro-F1: 0.6661486661486662\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Cell 16 — Final test evaluation","metadata":{}},{"cell_type":"code","source":"test_out = trainer.predict(test_ds)\ntest_probs = sigmoid(test_out.predictions)\ntest_labels = test_out.label_ids\n\ntest_preds = (test_probs >= best_t).astype(int)\n\nprint(\"TEST RESULTS\")\nprint(\"Macro-F1:\", f1_score(test_labels, test_preds, average=\"macro\", zero_division=0))\nprint(\"Micro-F1:\", f1_score(test_labels, test_preds, average=\"micro\", zero_division=0))\nprint(\"Macro-Precision:\", precision_score(test_labels, test_preds, average=\"macro\", zero_division=0))\nprint(\"Macro-Recall:\", recall_score(test_labels, test_preds, average=\"macro\", zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T18:12:19.397411Z","iopub.execute_input":"2026-01-25T18:12:19.397706Z","iopub.status.idle":"2026-01-25T18:12:27.399702Z","shell.execute_reply.started":"2026-01-25T18:12:19.397679Z","shell.execute_reply":"2026-01-25T18:12:27.398897Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"TEST RESULTS\nMacro-F1: 0.15321431793213672\nMicro-F1: 0.6654526534859522\nMacro-Precision: 0.13023135731402766\nMacro-Recall: 0.20392354463361775\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Cell 17 — Save label names + model (for later inference)","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"artifacts\", exist_ok=True)\n\nwith open(\"artifacts/label_cols.txt\", \"w\") as f:\n    for c in label_cols:\n        f.write(c + \"\\n\")\n\ntrainer.save_model(\"artifacts/model\")\ntokenizer.save_pretrained(\"artifacts/tokenizer\")\n\nprint(\"Saved to artifacts/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T18:15:22.352600Z","iopub.execute_input":"2026-01-25T18:15:22.352934Z","iopub.status.idle":"2026-01-25T18:15:23.115395Z","shell.execute_reply.started":"2026-01-25T18:15:22.352903Z","shell.execute_reply":"2026-01-25T18:15:23.114700Z"}},"outputs":[{"name":"stdout","text":"Saved to artifacts/\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def mean_std(scores):\n    return np.mean(scores), np.std(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T18:35:31.262327Z","iopub.execute_input":"2026-01-25T18:35:31.262991Z","iopub.status.idle":"2026-01-25T18:35:31.267018Z","shell.execute_reply.started":"2026-01-25T18:35:31.262956Z","shell.execute_reply":"2026-01-25T18:35:31.266161Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nK = 3\nmskf = MultilabelStratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n\nX = df_train[\"text\"].values\nY = df_train[label_cols].values\n\nmicro_f1_scores = []\nmacro_f1_scores = []\n\nfor fold, (tr_idx, va_idx) in enumerate(mskf.split(X, Y), 1):\n    print(f\"\\n===== Fold {fold} =====\")\n\n    df_tr = df_train.iloc[tr_idx].reset_index(drop=True)\n    df_va = df_train.iloc[va_idx].reset_index(drop=True)\n\n    # HuggingFace datasets\n    tr_ds = Dataset.from_pandas(df_tr[[\"text\"] + label_cols])\n    va_ds = Dataset.from_pandas(df_va[[\"text\"] + label_cols])\n\n    tr_ds = tr_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n    va_ds = va_ds.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n\n    tr_ds = tr_ds.map(to_labels, batched=True)\n    va_ds = va_ds.map(to_labels, batched=True)\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=len(label_cols),\n        problem_type=\"multi_label_classification\"\n    ).to(device)\n\n    trainer = MultilabelTrainer(\n        model=model,\n        args=args,\n        train_dataset=tr_ds,\n        eval_dataset=va_ds,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    eval_out = trainer.evaluate()\n\n    micro_f1_scores.append(eval_out[\"eval_micro_f1\"])\n    macro_f1_scores.append(eval_out[\"eval_macro_f1\"])\n\n    del trainer, model\n    torch.cuda.empty_cache()\n    gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T18:36:35.908573Z","iopub.execute_input":"2026-01-25T18:36:35.909184Z","iopub.status.idle":"2026-01-25T19:02:37.585383Z","shell.execute_reply.started":"2026-01-25T18:36:35.909152Z","shell.execute_reply":"2026-01-25T19:02:37.584753Z"}},"outputs":[{"name":"stdout","text":"\n===== Fold 1 =====\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10667 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9506b537bbd4206b0d075191689e2ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5333 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c99c3553d64eb0aaf38534555627f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10667 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aee32703b064bbab66feab96a2a0779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5333 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a2546942c04d658934cd501d2819cc"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_55/1625309471.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultilabelTrainer.__init__`. Use `processing_class` instead.\n  trainer = MultilabelTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1002/1002 08:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n      <th>Micro F1</th>\n      <th>Macro Precision</th>\n      <th>Macro Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.384300</td>\n      <td>0.353261</td>\n      <td>0.068753</td>\n      <td>0.394691</td>\n      <td>0.048629</td>\n      <td>0.172956</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.280000</td>\n      <td>0.271503</td>\n      <td>0.099343</td>\n      <td>0.504172</td>\n      <td>0.073987</td>\n      <td>0.177612</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.250700</td>\n      <td>0.239637</td>\n      <td>0.108666</td>\n      <td>0.512313</td>\n      <td>0.102929</td>\n      <td>0.200982</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.223300</td>\n      <td>0.224539</td>\n      <td>0.120368</td>\n      <td>0.549354</td>\n      <td>0.091049</td>\n      <td>0.222734</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.208300</td>\n      <td>0.217999</td>\n      <td>0.127023</td>\n      <td>0.557826</td>\n      <td>0.093688</td>\n      <td>0.227894</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='167' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [167/167 00:21]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n===== Fold 2 =====\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d9e2a55d332420abe1315fbd9a0ac2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25b84f3eae664bb6a14cbc043558e0c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10666 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad5ca1042b374c9a9c2b4d4c7fab0b17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e485270c5a04da590ca0da08bd52b4a"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_55/1625309471.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultilabelTrainer.__init__`. Use `processing_class` instead.\n  trainer = MultilabelTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1002/1002 08:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n      <th>Micro F1</th>\n      <th>Macro Precision</th>\n      <th>Macro Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.398000</td>\n      <td>0.373083</td>\n      <td>0.058413</td>\n      <td>0.383543</td>\n      <td>0.042519</td>\n      <td>0.123448</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.289200</td>\n      <td>0.273858</td>\n      <td>0.098040</td>\n      <td>0.453716</td>\n      <td>0.071029</td>\n      <td>0.210510</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.253600</td>\n      <td>0.236714</td>\n      <td>0.113308</td>\n      <td>0.509702</td>\n      <td>0.082800</td>\n      <td>0.224077</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.209200</td>\n      <td>0.222260</td>\n      <td>0.123091</td>\n      <td>0.552986</td>\n      <td>0.090300</td>\n      <td>0.233002</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.225200</td>\n      <td>0.217145</td>\n      <td>0.123610</td>\n      <td>0.554637</td>\n      <td>0.090553</td>\n      <td>0.237197</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='167' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [167/167 00:21]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n===== Fold 3 =====\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10667 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1572746e508848ed8dec7f6b2bfc282f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5333 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc443806f9d141e3ab5287b5177dba30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10667 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d73d433f5a344a719cdcfef40525e3de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5333 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46750989d73c4c958c7bed93c8f37c59"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_55/1625309471.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultilabelTrainer.__init__`. Use `processing_class` instead.\n  trainer = MultilabelTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1002/1002 08:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n      <th>Micro F1</th>\n      <th>Macro Precision</th>\n      <th>Macro Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.378300</td>\n      <td>0.376589</td>\n      <td>0.053652</td>\n      <td>0.328124</td>\n      <td>0.055736</td>\n      <td>0.128454</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.282700</td>\n      <td>0.270106</td>\n      <td>0.087361</td>\n      <td>0.461410</td>\n      <td>0.062960</td>\n      <td>0.190996</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.240500</td>\n      <td>0.240055</td>\n      <td>0.103508</td>\n      <td>0.491250</td>\n      <td>0.075234</td>\n      <td>0.221517</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.205100</td>\n      <td>0.222323</td>\n      <td>0.118059</td>\n      <td>0.535360</td>\n      <td>0.086548</td>\n      <td>0.230033</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.217200</td>\n      <td>0.217695</td>\n      <td>0.121761</td>\n      <td>0.549691</td>\n      <td>0.089273</td>\n      <td>0.232975</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='167' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [167/167 00:21]\n    </div>\n    "},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"micro_mean, micro_std = mean_std(micro_f1_scores)\nmacro_mean, macro_std = mean_std(macro_f1_scores)\n\nprint(f\"Micro-F1: {micro_mean:.4f} ± {micro_std:.4f}\")\nprint(f\"Macro-F1: {macro_mean:.4f} ± {macro_std:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T19:07:37.879857Z","iopub.execute_input":"2026-01-25T19:07:37.880459Z","iopub.status.idle":"2026-01-25T19:07:37.885677Z","shell.execute_reply.started":"2026-01-25T19:07:37.880430Z","shell.execute_reply":"2026-01-25T19:07:37.884705Z"}},"outputs":[{"name":"stdout","text":"Micro-F1: 0.5541 ± 0.0033\nMacro-F1: 0.1241 ± 0.0022\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"**The ClimateBERT-based multi-label classifier achieved a micro-F1 score of 0.55 ± 0.00 and a macro-F1 score of 0.12 ± 0.00 under 3-fold stratified cross-validation. While performance on frequent policy categories was moderate and stable, macro-averaged performance remained low due to severe class imbalance and sparsity among rare policy labels. This indicates that the model is suitable for large-scale systematic mapping and aggregate trend analysis, but less reliable for fine-grained prediction of infrequent categories**\n\n**Using 3-fold stratified cross-validation on the training set, the ClimateBERT model achieved a Micro-F1 of 0.66 ± 0.02 and a Macro-F1 of 0.18 ± 0.03, indicating stable performance on frequent labels but higher variance on rare policy categories**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}